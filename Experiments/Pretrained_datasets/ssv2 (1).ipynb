{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "155a0c20-70e8-4792-be1d-d01746d59530",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "project_path = '/user/HS402/zs00774/Downloads'  # Update with your project path\n",
    "os.chdir(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d712d73f-cca9-4a35-a1e5-70ec5a533782",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: flask in /user/HS402/zs00774/.local/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (3.1.0)\n",
      "Requirement already satisfied: torch in /user/HS402/zs00774/.local/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (2.7.0)\n",
      "Requirement already satisfied: torchvision in /user/HS402/zs00774/.local/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (0.22.0)\n",
      "Requirement already satisfied: transformers in /user/HS402/zs00774/.local/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (4.51.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.26.4)\n",
      "Requirement already satisfied: opencv-python in /user/HS402/zs00774/.local/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (4.11.0.86)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.5.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (3.7.2)\n",
      "Requirement already satisfied: tensorboard in /user/HS402/zs00774/.local/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (2.19.0)\n",
      "Requirement already satisfied: Pillow in /usr/lib/python3/dist-packages (from -r requirements.txt (line 10)) (9.0.1)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (0.13.2)\n",
      "Requirement already satisfied: blinker>=1.9 in /user/HS402/zs00774/.local/lib/python3.10/site-packages (from flask->-r requirements.txt (line 1)) (1.9.0)\n",
      "Requirement already satisfied: itsdangerous>=2.2 in /user/HS402/zs00774/.local/lib/python3.10/site-packages (from flask->-r requirements.txt (line 1)) (2.2.0)\n",
      "Requirement already satisfied: Werkzeug>=3.1 in /user/HS402/zs00774/.local/lib/python3.10/site-packages (from flask->-r requirements.txt (line 1)) (3.1.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /user/HS402/zs00774/.local/lib/python3.10/site-packages (from flask->-r requirements.txt (line 1)) (3.1.6)\n",
      "Requirement already satisfied: click>=8.1.3 in /user/HS402/zs00774/.local/lib/python3.10/site-packages (from flask->-r requirements.txt (line 1)) (8.1.8)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /user/HS402/zs00774/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /user/HS402/zs00774/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /user/HS402/zs00774/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (12.6.80)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 2)) (4.12.2)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from torch->-r requirements.txt (line 2)) (3.6.0)\n",
      "Requirement already satisfied: triton==3.3.0 in /user/HS402/zs00774/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (3.3.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /user/HS402/zs00774/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /user/HS402/zs00774/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /user/HS402/zs00774/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /user/HS402/zs00774/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (10.3.7.77)\n",
      "Requirement already satisfied: networkx in /usr/lib/python3/dist-packages (from torch->-r requirements.txt (line 2)) (2.4)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /user/HS402/zs00774/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (11.3.0.4)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /user/HS402/zs00774/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (1.14.0)\n",
      "Requirement already satisfied: fsspec in /user/HS402/zs00774/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /user/HS402/zs00774/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (1.11.1.6)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /user/HS402/zs00774/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /user/HS402/zs00774/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (0.6.3)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /user/HS402/zs00774/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /user/HS402/zs00774/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /user/HS402/zs00774/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (12.6.85)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/lib/python3/dist-packages (from triton==3.3.0->torch->-r requirements.txt (line 2)) (59.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 4)) (24.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 4)) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /user/HS402/zs00774/.local/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 4)) (4.67.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers->-r requirements.txt (line 4)) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /user/HS402/zs00774/.local/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 4)) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /user/HS402/zs00774/.local/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 4)) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /user/HS402/zs00774/.local/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 4)) (0.5.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /user/HS402/zs00774/.local/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 4)) (0.30.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (3.5.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (1.11.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (1.4.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/lib/python3/dist-packages (from matplotlib->-r requirements.txt (line 8)) (4.29.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/lib/python3/dist-packages (from matplotlib->-r requirements.txt (line 8)) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 8)) (1.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 8)) (2.9.0.post0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/lib/python3/dist-packages (from matplotlib->-r requirements.txt (line 8)) (1.3.2)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 8)) (3.0.9)\n",
      "Requirement already satisfied: six>1.9 in /usr/lib/python3/dist-packages (from tensorboard->-r requirements.txt (line 9)) (1.16.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /user/HS402/zs00774/.local/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 9)) (2.2.0)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /user/HS402/zs00774/.local/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 9)) (5.29.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /user/HS402/zs00774/.local/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 9)) (0.7.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /user/HS402/zs00774/.local/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 9)) (3.7)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /user/HS402/zs00774/.local/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 9)) (1.71.0)\n",
      "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn->-r requirements.txt (line 11)) (2.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /user/HS402/zs00774/.local/lib/python3.10/site-packages (from Jinja2>=3.1.2->flask->-r requirements.txt (line 1)) (3.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas>=1.2->seaborn->-r requirements.txt (line 11)) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn->-r requirements.txt (line 11)) (2024.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /user/HS402/zs00774/.local/lib/python3.10/site-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers->-r requirements.txt (line 4)) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers->-r requirements.txt (line 4)) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers->-r requirements.txt (line 4)) (2020.6.20)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 4)) (3.3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f3fc946-0fd5-4835-ac15-859a3b11cfd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-11 02:25:58.066135: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-11 02:25:58.072417: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754875558.080747   95518 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754875558.083257   95518 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754875558.089686   95518 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754875558.089693   95518 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754875558.089694   95518 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754875558.089695   95518 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-11 02:25:58.092158: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from transformers import Trainer, TrainingArguments,EarlyStoppingCallback\n",
    "from transformers.trainer_utils import IntervalStrategy\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)  # Suppress FutureWarnings\n",
    "from torch import nn, optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.cuda.amp import GradScaler, autocast  # For mixed precision training\n",
    "import logging\n",
    "import sys\n",
    "import argparse\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import AutoImageProcessor, AutoModelForVideoClassification\n",
    "import os\n",
    "from transformers import TimesformerForVideoClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73c8a362-527c-430a-a61f-07beef41cd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total clips: 900\n",
      "Total clips: 225\n",
      "Total clips: 125\n",
      "Train: 900 | Val: 225 | Test: 125\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class HMDBDataset(Dataset):\n",
    "    def __init__(self, split_dir, clip_size=8, frame_rate=32, transform=None):\n",
    "        \"\"\"\n",
    "        Dataset that loads from pre-split folder structure\n",
    "        \n",
    "        Args:\n",
    "            split_dir (str): Path to split directory (train, val, or test folder)\n",
    "            clip_size (int): Number of frames per clip\n",
    "            frame_rate (int): Sampling rate (every Nth frame)\n",
    "            transform: Torchvision transforms\n",
    "        \"\"\"\n",
    "        self.split_dir = split_dir\n",
    "        self.clip_size = clip_size\n",
    "        self.frame_rate = frame_rate\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Get all class folders in the split directory\n",
    "        self.classes = sorted([d for d in os.listdir(split_dir) \n",
    "                              if os.path.isdir(os.path.join(split_dir, d))])\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        \n",
    "        \n",
    "        self.data = self._load_data()\n",
    "\n",
    "    def _interpolate_frames(self, frames_list):\n",
    "        \"\"\"Add interpolated frames between consecutive frames\"\"\"\n",
    "        if len(frames_list) < 2:\n",
    "            return frames_list\n",
    "        \n",
    "        new_frames = []\n",
    "        for i in range(len(frames_list) - 1):\n",
    "            new_frames.append(frames_list[i])\n",
    "            # Add interpolated frame between current and next\n",
    "            interp_name = f\"interp_{frames_list[i]}_{frames_list[i+1]}\"\n",
    "            new_frames.append(interp_name)\n",
    "        new_frames.append(frames_list[-1])  # Add last frame\n",
    "        return new_frames\n",
    "    \n",
    "    def _reverse_frames(self, frames_list):\n",
    "        \"\"\"Add temporally reversed frames\"\"\"\n",
    "        reversed_frames = frames_list[::-1][1:]\n",
    "        return frames_list + reversed_frames\n",
    "    \n",
    "    def _brightness_augment_frames(self, frames_list):\n",
    "        \"\"\"Add brightness-varied versions of frames\"\"\"\n",
    "        brightness_levels = [0.6, 0.8, 1.2, 1.4]\n",
    "        new_frames = frames_list.copy()\n",
    "        \n",
    "        for brightness in brightness_levels:\n",
    "            for frame in frames_list:\n",
    "                bright_frame = f\"bright_{brightness}_{frame}\"\n",
    "                new_frames.append(bright_frame)\n",
    "        \n",
    "        return new_frames\n",
    "    \n",
    "    \n",
    "    def _augment_frames_sequence(self, frames_list, target_count):\n",
    "        \"\"\"\n",
    "        Apply augmentation techniques in sequence until we reach target_count frames\n",
    "        \"\"\"\n",
    "        if len(frames_list) == 0:\n",
    "            return ['black_frame'] * target_count\n",
    "        \n",
    "        augmented_frames = frames_list.copy()\n",
    "        augmentation_methods = [\n",
    "            self._interpolate_frames,\n",
    "            self._reverse_frames, \n",
    "            self._brightness_augment_frames\n",
    "        ]\n",
    "        \n",
    "        max_iterations = 5\n",
    "        iteration = 0\n",
    "        \n",
    "        while len(augmented_frames) < target_count and iteration < max_iterations:\n",
    "            iteration += 1\n",
    "            initial_count = len(augmented_frames)\n",
    "            \n",
    "            # Apply each augmentation method in sequence\n",
    "            for aug_method in augmentation_methods:\n",
    "                if len(augmented_frames) >= target_count:\n",
    "                    break\n",
    "                augmented_frames = aug_method(augmented_frames)\n",
    "            \n",
    "            # Break if no progress was made\n",
    "            if len(augmented_frames) == initial_count:\n",
    "                break\n",
    "        \n",
    "        return augmented_frames[:target_count]\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"Load data from the split directory\"\"\"\n",
    "        data = []\n",
    "        \n",
    "        for class_name in self.classes:\n",
    "            class_path = os.path.join(self.split_dir, class_name)\n",
    "            label = self.class_to_idx[class_name]\n",
    "            \n",
    "            # Get all video folders in this class\n",
    "            video_folders = [v for v in os.listdir(class_path) \n",
    "                           if os.path.isdir(os.path.join(class_path, v))]\n",
    "            \n",
    "            for video_folder in video_folders:\n",
    "                video_path = os.path.join(class_path, video_folder)\n",
    "                \n",
    "                # Skip already processed folders\n",
    "                if '_' in video_folder and video_folder.split('_')[-1].isdigit():\n",
    "                    continue\n",
    "                \n",
    "                # Get all image frames (unsampled)\n",
    "                all_frames = sorted([f for f in os.listdir(video_path) \n",
    "                                   if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))])\n",
    "                \n",
    "                if len(all_frames) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Check if we need augmentation\n",
    "                required_frames = self.frame_rate * self.clip_size  # e.g., 32 * 8 = 256\n",
    "                \n",
    "                if len(all_frames) < required_frames:\n",
    "                    # Apply augmentation to get enough frames\n",
    "                    augmented_frames = self._augment_frames_sequence(all_frames, required_frames)\n",
    "                    working_frames = augmented_frames\n",
    "                else:\n",
    "                    working_frames = all_frames\n",
    "                \n",
    "                # Now sample from the working frames\n",
    "                sampled_frames = working_frames[::self.frame_rate]\n",
    "                \n",
    "                # Create single clip per video\n",
    "                if len(sampled_frames) >= self.clip_size:\n",
    "                    # Take the first clip_size frames for the single clip\n",
    "                    clip_frames = sampled_frames[:self.clip_size]\n",
    "                else:\n",
    "                    # This should be rare since we augmented first, but handle it\n",
    "                    # Repeat frames to reach clip_size\n",
    "                    while len(sampled_frames) < self.clip_size:\n",
    "                        sampled_frames.extend(sampled_frames)\n",
    "                    clip_frames = sampled_frames[:self.clip_size]\n",
    "                \n",
    "                # Add single clip to dataset\n",
    "                data.append((video_path, label, 0, clip_frames))\n",
    "                \n",
    "        print(f\"Total clips: {len(data)}\")\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _get_fallback_frame(self, video_path):\n",
    "        \"\"\"Get a fallback frame when specific frame loading fails\"\"\"\n",
    "        available_frames = [f for f in os.listdir(video_path) \n",
    "                          if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
    "        if available_frames:\n",
    "            return Image.open(os.path.join(video_path, available_frames[0]))\n",
    "        else:\n",
    "            return Image.fromarray(np.zeros((224, 224, 3), dtype=np.uint8))\n",
    "\n",
    "    def _load_regular_frame(self, video_path, frame_name):\n",
    "        \"\"\"Load a regular frame from disk\"\"\"\n",
    "        frame_path = os.path.join(video_path, frame_name)\n",
    "        if os.path.exists(frame_path):\n",
    "            try:\n",
    "                return Image.open(frame_path)\n",
    "            except Exception as e:\n",
    "                return self._get_fallback_frame(video_path)\n",
    "        else:\n",
    "            return self._get_fallback_frame(video_path)\n",
    "\n",
    "    def _create_interpolated_frame(self, video_path, frame_name):\n",
    "        \"\"\"Create interpolated frame from two source frames\"\"\"\n",
    "        # Format: \"interp_frame1_frame2\"\n",
    "        parts = frame_name.replace('interp_', '').split('_')\n",
    "        if len(parts) >= 2:\n",
    "            frame1_name = '_'.join(parts[:-1])\n",
    "            frame2_name = parts[-1]\n",
    "            \n",
    "            # Recursively process the source frames (handles nested cases)\n",
    "            img1 = self._process_frame(video_path, frame1_name)\n",
    "            img2 = self._process_frame(video_path, frame2_name)\n",
    "            \n",
    "            try:\n",
    "                img1_array = np.array(img1)\n",
    "                img2_array = np.array(img2)\n",
    "                # Simple interpolation\n",
    "                img_avg = np.mean([img1_array, img2_array], axis=0, dtype=np.uint8)\n",
    "                return Image.fromarray(img_avg)\n",
    "            except Exception as e:\n",
    "                return img  # Fallback to original\n",
    "        else:\n",
    "            return self._get_fallback_frame(video_path)\n",
    "\n",
    "    def _create_brightness_frame(self, video_path, frame_name):\n",
    "        \"\"\"Create brightness-adjusted frame\"\"\"\n",
    "        # Format: \"bright_1.2_original_frame.jpg\"\n",
    "        parts = frame_name.split('_', 2)\n",
    "        if len(parts) >= 3:\n",
    "            brightness_factor = float(parts[1])\n",
    "            original_frame = parts[2]\n",
    "            \n",
    "            # Recursively process the source frame (handles nested cases)\n",
    "            img = self._process_frame(video_path, original_frame)\n",
    "            \n",
    "            try:\n",
    "                img_np = np.asarray(img).astype(np.float32)\n",
    "                img_np = img_np * brightness_factor\n",
    "                img_np = np.clip(img_np, 0, 255).astype(np.uint8)\n",
    "                return Image.fromarray(img_np)\n",
    "            except Exception as e:\n",
    "                return img  # Fallback to original\n",
    "        else:\n",
    "            return self._get_fallback_frame(video_path)\n",
    "\n",
    "    def _process_frame(self, video_path, frame_name):\n",
    "        \"\"\"Process individual frame based on its name/type\"\"\"\n",
    "        if frame_name == 'black_frame':\n",
    "            return Image.fromarray(np.zeros((224, 224, 3), dtype=np.uint8))\n",
    "        elif frame_name.startswith('interp_'):\n",
    "            return self._create_interpolated_frame(video_path, frame_name)\n",
    "        elif frame_name.startswith('bright_'):\n",
    "            return self._create_brightness_frame(video_path, frame_name)\n",
    "        else:\n",
    "            return self._load_regular_frame(video_path, frame_name)\n",
    "\n",
    "    def _load_frames_from_clip(self, video_path, clip_frames):\n",
    "        \"\"\"Load frames for a specific clip with simplified processing\"\"\"\n",
    "        frames = []\n",
    "        \n",
    "        for frame_name in clip_frames:\n",
    "            img = self._process_frame(video_path, frame_name)\n",
    "            frames.append(img)\n",
    "        \n",
    "        # Ensure exactly clip_size frames\n",
    "        while len(frames) < self.clip_size:\n",
    "            if frames:\n",
    "                frames.append(frames[-1])  # Repeat last frame\n",
    "            else:\n",
    "                frames.append(Image.fromarray(np.zeros((224, 224, 3), dtype=np.uint8)))\n",
    "        \n",
    "        frames = frames[:self.clip_size]\n",
    "        return frames\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path, label, clip_idx, clip_frames = self.data[idx]\n",
    "        \n",
    "        # Load frames for this specific clip\n",
    "        frames = self._load_frames_from_clip(video_path, clip_frames)\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            frames = [self.transform(frame) for frame in frames]\n",
    "        \n",
    "        return torch.stack(frames), label\n",
    "\n",
    "\n",
    "def get_dataloader(root_dir, batch_size=8, clip_size=8, frame_rate=32):\n",
    "    \"\"\"\n",
    "    Create dataloaders for pre-split dataset folders\n",
    "    \n",
    "    Args:\n",
    "        root_dir (str): Path to directory containing train, val, test folders\n",
    "        batch_size (int): Batch size for dataloaders\n",
    "        clip_size (int): Number of frames per clip\n",
    "        frame_rate (int): Sampling rate (every Nth frame)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_loader, val_loader, test_loader)\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Paths to split directories\n",
    "    train_dir = os.path.join(root_dir, 'train')\n",
    "    val_dir = os.path.join(root_dir, 'val')\n",
    "    test_dir = os.path.join(root_dir, 'test')\n",
    "    \n",
    "    # Check if directories exist\n",
    "    for split_dir, name in [(train_dir, 'train'), (val_dir, 'val'), (test_dir, 'test')]:\n",
    "        if not os.path.exists(split_dir):\n",
    "            raise ValueError(f\"{name} directory not found at {split_dir}\")\n",
    "    \n",
    "    # Create datasets for each split\n",
    "    train_dataset = HMDBDataset(\n",
    "        split_dir=train_dir,\n",
    "        clip_size=clip_size, \n",
    "        frame_rate=frame_rate,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    val_dataset = HMDBDataset(\n",
    "        split_dir=val_dir,\n",
    "        clip_size=clip_size, \n",
    "        frame_rate=frame_rate,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    test_dataset = HMDBDataset(\n",
    "        split_dir=test_dir,\n",
    "        clip_size=clip_size, \n",
    "        frame_rate=frame_rate,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, \n",
    "        num_workers=0, pin_memory=False\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=0, pin_memory=False\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=0, pin_memory=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Train: {len(train_dataset)} | Val: {len(val_dataset)} | Test: {len(test_dataset)}\")\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    root_dir = \"/user/HS402/zs00774/Downloads/dataset\"\n",
    "    train_loader, val_loader, test_loader = get_dataloader(root_dir=root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d03e307-d014-4a57-bfcf-83b95d29d098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_timesformer_model():\n",
    "    \"\"\"\n",
    "    Load the pre-trained TimeSformer model for video classification.\n",
    "\n",
    "    \"\"\"\n",
    "    import logging\n",
    "    warnings.filterwarnings(\"ignore\", message=\"Some weights of TimesformerForVideoClassification were not initialized\")\n",
    "    logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\n",
    "\n",
    "    label_index_dict={'brush_hair': 0, 'cartwheel': 1, 'catch': 2, 'chew': 3, 'climb': 4, 'climb_stairs': 5, 'draw_sword': 6, 'eat': 7, 'fencing': 8, 'flic_flac': 9, 'golf': 10, 'handstand': 11, 'kiss': 12, 'pick': 13, 'pour': 14, 'pullup': 15, 'pushup': 16, 'ride_bike': 17, 'shoot_bow': 18, 'shoot_gun': 19, 'situp': 20, 'smile': 21, 'smoke': 22, 'throw': 23, 'wave': 24}\n",
    "    index_label_dict={0: 'brush_hair', 1: 'cartwheel', 2: 'catch', 3: 'chew', 4: 'climb', 5: 'climb_stairs', 6: 'draw_sword', 7: 'eat', 8: 'fencing', 9: 'flic_flac',10: 'golf', 11: 'handstand', 12: 'kiss', 13: 'pick', 14: 'pour', 15: 'pullup', 16: 'pushup', 17: 'ride_bike', 18: 'shoot_bow', 19: 'shoot_gun', 20: 'situp', 21: 'smile', 22: 'smoke', 23: 'throw', 24: 'wave'}\n",
    "    # Load the processor and model from Hugging Face\n",
    "    processor =AutoImageProcessor.from_pretrained(\"facebook/timesformer-base-finetuned-ssv2\")\n",
    "    ckpt = \"facebook/timesformer-base-finetuned-ssv2\"\n",
    "    model = TimesformerForVideoClassification.from_pretrained(ckpt,label2id = label_index_dict,id2label = index_label_dict,ignore_mismatched_sizes = True)\n",
    "\n",
    "    return processor, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a87f6bb-cf9f-4499-bbd2-2f9670ca6bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    if isinstance(predictions, torch.Tensor):\n",
    "        predictions = predictions.cpu().numpy()\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.cpu().numpy()\n",
    "\n",
    "    # Top-1 accuracy\n",
    "    top1_predictions = np.argmax(predictions, axis=1)\n",
    "    top1_accuracy = np.mean(top1_predictions == labels)\n",
    "\n",
    "    # Top-5 accuracy\n",
    "    top5_predictions = np.argsort(predictions, axis=1)[:, -5:]\n",
    "    top5_correct = np.array([labels[i] in top5_predictions[i] for i in range(len(labels))])\n",
    "    top5_accuracy = np.mean(top5_correct)\n",
    "\n",
    "    return {\n",
    "        \"top1_accuracy\": top1_accuracy,\n",
    "        \"top5_accuracy\": top5_accuracy,\n",
    "        \"eval_top1_accuracy\": top1_accuracy,\n",
    "        \"eval_top5_accuracy\": top5_accuracy,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cc7a0a2-c86c-4fb5-a37f-4236ec23341b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_trainer(data_dir, epochs, batch_size, learning_rate):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    train_loader, val_loader, test_loader = get_dataloader(data_dir, batch_size)\n",
    "    processor, model = load_timesformer_model()\n",
    "\n",
    "    train_dataset = train_loader.dataset\n",
    "    val_dataset = val_loader.dataset\n",
    "\n",
    "    def data_collator(batch):\n",
    "        videos = torch.stack([item[0] for item in batch])\n",
    "        labels = torch.tensor([item[1] for item in batch])\n",
    "        return {\n",
    "            'pixel_values': videos,\n",
    "            'labels': labels\n",
    "        }\n",
    "    optimizer = optim.SGD(model.parameters(), momentum= 0.9, weight_decay= 1e-3,\n",
    "    \t\t\t  lr= learning_rate)\n",
    "\n",
    "    training_args = TrainingArguments(output_dir = \"/user/HS402/zs00774/Downloads/results\",\n",
    "                                      overwrite_output_dir = True,\n",
    "                                      eval_strategy = 'epoch',\n",
    "                                      per_device_train_batch_size = 8,\n",
    "                                      per_device_eval_batch_size = 8,\n",
    "                                      num_train_epochs = epochs,\n",
    "                                      logging_dir = os.path.normpath(os.path.join('/user/HS402/zs00774/Downloads/results', 'logs')),\n",
    "                                      logging_strategy = \"epoch\",\n",
    "                                      save_strategy = 'epoch',\n",
    "                                      save_total_limit = 1,\n",
    "                                      remove_unused_columns = False,\n",
    "                                      load_best_model_at_end = True,\n",
    "                                      metric_for_best_model = 'eval_top1_accuracy',\n",
    "                                      greater_is_better = True,\n",
    "                                      report_to = \"tensorboard\",\n",
    "                                      push_to_hub = False,\n",
    "                                     save_only_model = True)\n",
    "\n",
    "    early_stopping_callback = EarlyStoppingCallback(early_stopping_patience= 4 )\n",
    "\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        optimizers=(optimizer,None),\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks = [early_stopping_callback])\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_dataset = test_loader.dataset\n",
    "    test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "\n",
    "    print(f\"\\n=== FINAL TEST RESULTS ===\")\n",
    "    print(f\"Test Top-1 Accuracy: {test_results.get('eval_top1_accuracy', 'N/A'):.4f}\")\n",
    "    print(f\"Test Top-5 Accuracy: {test_results.get('eval_top5_accuracy', 'N/A'):.4f}\")\n",
    "    print(f\"Test Loss: {test_results.get('eval_loss', 'N/A'):.4f}\")\n",
    "\n",
    "    trainer.save_model(\"/user/HS402/zs00774/Downloads/timesformer_model\")\n",
    "\n",
    "    with open(\"/user/HS402/zs00774/Downloads/test_results.json\", \"w\") as f:\n",
    "        json.dump(test_results, f, indent=2)\n",
    "\n",
    "    print(\"Training complete. Model and results saved.\")\n",
    "    return test_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d07c4189-a107-43e0-bfeb-111ccc2bfa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_saved_model(model_path, data_dir, batch_size=8):\n",
    "    from transformers import TimesformerForVideoClassification\n",
    "\n",
    "    model = TimesformerForVideoClassification.from_pretrained(model_path)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    _, _, test_loader = get_dataloader(data_dir, batch_size)\n",
    "\n",
    "    def data_collator(batch):\n",
    "        videos = torch.stack([item[0] for item in batch])\n",
    "        labels = torch.tensor([item[1] for item in batch])\n",
    "        return {\n",
    "            'pixel_values': videos,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    results = trainer.evaluate(eval_dataset=test_loader.dataset)\n",
    "\n",
    "    print(f\"=== Model Evaluation: {model_path} ===\")\n",
    "    print(f\"Top-1 Accuracy: {results.get('eval_top1_accuracy', 'N/A'):.4f}\")\n",
    "    print(f\"Top-5 Accuracy: {results.get('eval_top5_accuracy', 'N/A'):.4f}\")\n",
    "    print(f\"Loss: {results.get('eval_loss', 'N/A'):.4f}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a511255-1333-4cc3-9818-6ec90bd1fc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Total clips: 900\n",
      "Total clips: 225\n",
      "Total clips: 125\n",
      "Train: 900 | Val: 225 | Test: 125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4b72f56536f4e5a84528c12b687406e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/412 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af517c0c8c7042209ac7621378ecdf83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d625cdd1c96a45d4b94a235139062d6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/486M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcfcaa2456334ce694fe2e890d15882f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/486M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='904' max='904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [904/904 27:04, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Top1 Accuracy</th>\n",
       "      <th>Top5 Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.773600</td>\n",
       "      <td>2.117869</td>\n",
       "      <td>0.537778</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.573800</td>\n",
       "      <td>1.422929</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.974000</td>\n",
       "      <td>1.110342</td>\n",
       "      <td>0.742222</td>\n",
       "      <td>0.951111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.659300</td>\n",
       "      <td>0.949879</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.484000</td>\n",
       "      <td>0.864312</td>\n",
       "      <td>0.773333</td>\n",
       "      <td>0.964444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.377800</td>\n",
       "      <td>0.816488</td>\n",
       "      <td>0.782222</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.315200</td>\n",
       "      <td>0.791674</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.282000</td>\n",
       "      <td>0.784478</td>\n",
       "      <td>0.791111</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FINAL TEST RESULTS ===\n",
      "Test Top-1 Accuracy: 0.8480\n",
      "Test Top-5 Accuracy: 0.9680\n",
      "Test Loss: 0.6033\n",
      "Training complete. Model and results saved.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_top1_accuracy': 0.848,\n",
       " 'eval_top5_accuracy': 0.968,\n",
       " 'eval_loss': 0.6032775044441223,\n",
       " 'eval_runtime': 10.468,\n",
       " 'eval_samples_per_second': 11.941,\n",
       " 'eval_steps_per_second': 1.528,\n",
       " 'epoch': 8.0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model_with_trainer(\n",
    "        data_dir='/user/HS402/zs00774/Downloads/dataset',\n",
    "        epochs=8,\n",
    "        batch_size=8,\n",
    "        learning_rate=0.0035\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb8cb70-1b5e-4e1c-9f4b-a0dc3f3a749f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769f14e4-2759-43ae-acff-2d6045f1fdb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f30702-8806-4199-87c1-935e6dc828c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb447ac-cd73-4a67-a74b-111f341ec104",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
