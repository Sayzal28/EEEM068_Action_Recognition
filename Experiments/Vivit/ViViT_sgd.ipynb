{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "# Unzip dataset\n",
        "zip_path = '/content/drive/MyDrive/AML/HMDB_simp.zip'\n",
        "extract_path = '/content/drive/MyDrive/AML'\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(f\"Dataset extracted to {extract_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ER7rzNnCsHqB",
        "outputId": "688b3c15-afb9-4337-bdc6-1b550ae852c6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset extracted to /content/drive/MyDrive/AML\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import json\n",
        "from transformers import Trainer, TrainingArguments,EarlyStoppingCallback\n",
        "from transformers.trainer_utils import IntervalStrategy\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)  # Suppress FutureWarnings\n",
        "from torch import nn, optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.cuda.amp import GradScaler, autocast  # For mixed precision training\n",
        "import logging\n",
        "import sys\n",
        "import argparse\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from transformers import AutoImageProcessor, AutoModelForVideoClassification\n",
        "import os\n",
        "from transformers import TimesformerForVideoClassification"
      ],
      "metadata": {
        "id": "kiclwab0O5ez"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8XD1aZeao7U2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05458a4a-c5d0-42b7-e2d4-c97d4f52b939"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying 900 videos...\n",
            "Copied 900/900 videos\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "import pandas as pd\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# Paths\n",
        "original_root = \"/content/drive/MyDrive/AML/HMDB_simp\"\n",
        "subset_root = \"/content/drive/MyDrive/AML/dataset/train\"\n",
        "csv_path = \"/content/drive/MyDrive/AML/split/train.csv\"\n",
        "\n",
        "def copy_video(row):\n",
        "    \"\"\"Copy a single video\"\"\"\n",
        "    label, video = row['class'], row['video_name']\n",
        "\n",
        "    src_path = os.path.join(original_root, label, video)\n",
        "    dst_path = os.path.join(subset_root, label, video)\n",
        "\n",
        "    if os.path.exists(src_path):\n",
        "        os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
        "        shutil.copytree(src_path, dst_path, dirs_exist_ok=True)\n",
        "        return True\n",
        "    else:\n",
        "        print(f\"Warning: {src_path} does not exist.\")\n",
        "        return False\n",
        "\n",
        "# Load CSV and copy files\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Method 1: Sequential (simple)\n",
        "def sequential_copy():\n",
        "    success_count = 0\n",
        "    for _, row in df.iterrows():\n",
        "        if copy_video(row):\n",
        "            success_count += 1\n",
        "    print(f\"Copied {success_count}/{len(df)} videos\")\n",
        "\n",
        "# Method 2: Parallel (faster)\n",
        "def parallel_copy(max_workers=8):\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        results = list(executor.map(copy_video, [row for _, row in df.iterrows()]))\n",
        "\n",
        "    success_count = sum(results)\n",
        "    print(f\"Copied {success_count}/{len(df)} videos\")\n",
        "\n",
        "# Run parallel copy\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"Copying {len(df)} videos...\")\n",
        "    parallel_copy()\n",
        "    print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import pandas as pd\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# Paths\n",
        "original_root = \"/content/drive/MyDrive/AML/HMDB_simp\"\n",
        "subset_root = \"/content/drive/MyDrive/AML/dataset/test\"\n",
        "csv_path = \"/content/drive/MyDrive/AML/split/test.csv\"\n",
        "\n",
        "def copy_video(row):\n",
        "    \"\"\"Copy a single video\"\"\"\n",
        "    label, video = row['class'], row['video_name']\n",
        "\n",
        "    src_path = os.path.join(original_root, label, video)\n",
        "    dst_path = os.path.join(subset_root, label, video)\n",
        "\n",
        "    if os.path.exists(src_path):\n",
        "        os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
        "        shutil.copytree(src_path, dst_path, dirs_exist_ok=True)\n",
        "        return True\n",
        "    else:\n",
        "        print(f\"Warning: {src_path} does not exist.\")\n",
        "        return False\n",
        "\n",
        "# Load CSV and copy files\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Method 1: Sequential (simple)\n",
        "def sequential_copy():\n",
        "    success_count = 0\n",
        "    for _, row in df.iterrows():\n",
        "        if copy_video(row):\n",
        "            success_count += 1\n",
        "    print(f\"Copied {success_count}/{len(df)} videos\")\n",
        "\n",
        "# Method 2: Parallel (faster)\n",
        "def parallel_copy(max_workers=8):\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        results = list(executor.map(copy_video, [row for _, row in df.iterrows()]))\n",
        "\n",
        "    success_count = sum(results)\n",
        "    print(f\"Copied {success_count}/{len(df)} videos\")\n",
        "\n",
        "# Run parallel copy\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"Copying {len(df)} videos...\")\n",
        "    parallel_copy()\n",
        "    print(\"Done!\")"
      ],
      "metadata": {
        "id": "OXF05toFEnfD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33893fa9-b25c-4a30-af98-bead6ec36a10"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying 125 videos...\n",
            "Copied 125/125 videos\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import pandas as pd\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# Paths\n",
        "original_root = \"/content/drive/MyDrive/AML/HMDB_simp\"\n",
        "subset_root = \"/content/drive/MyDrive/AML/dataset/val\"\n",
        "csv_path = \"/content/drive/MyDrive/AML/split/val.csv\"\n",
        "\n",
        "def copy_video(row):\n",
        "    \"\"\"Copy a single video\"\"\"\n",
        "    label, video = row['class'], row['video_name']\n",
        "\n",
        "    src_path = os.path.join(original_root, label, video)\n",
        "    dst_path = os.path.join(subset_root, label, video)\n",
        "\n",
        "    if os.path.exists(src_path):\n",
        "        os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
        "        shutil.copytree(src_path, dst_path, dirs_exist_ok=True)\n",
        "        return True\n",
        "    else:\n",
        "        print(f\"Warning: {src_path} does not exist.\")\n",
        "        return False\n",
        "\n",
        "# Load CSV and copy files\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Method 1: Sequential (simple)\n",
        "def sequential_copy():\n",
        "    success_count = 0\n",
        "    for _, row in df.iterrows():\n",
        "        if copy_video(row):\n",
        "            success_count += 1\n",
        "    print(f\"Copied {success_count}/{len(df)} videos\")\n",
        "\n",
        "# Method 2: Parallel (faster)\n",
        "def parallel_copy(max_workers=8):\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        results = list(executor.map(copy_video, [row for _, row in df.iterrows()]))\n",
        "\n",
        "    success_count = sum(results)\n",
        "    print(f\"Copied {success_count}/{len(df)} videos\")\n",
        "\n",
        "# Run parallel copy\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"Copying {len(df)} videos...\")\n",
        "    parallel_copy()\n",
        "    print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QR_H8SnVMvS6",
        "outputId": "d0aaa3f6-2829-422b-a3c1-b2e7b8bb5a24"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying 225 videos...\n",
            "Copied 225/225 videos\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import json\n",
        "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "from transformers import VivitImageProcessor, VivitForVideoClassification\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import random\n",
        "\n",
        "class ViViTHMDBDataset(Dataset):\n",
        "    def __init__(self, split_dir, clip_size=32, sampling_strategy='random', transform=None):\n",
        "        \"\"\"\n",
        "        Dataset for ViViT model\n",
        "\n",
        "        Args:\n",
        "            split_dir (str): Path to split directory (train, val, or test folder)\n",
        "            clip_size (int): Number of frames per clip (ViViT typically uses 32)\n",
        "            sampling_strategy (str): 'random' or 'equidistant'\n",
        "            transform: Torchvision transforms\n",
        "        \"\"\"\n",
        "        self.split_dir = split_dir\n",
        "        self.clip_size = clip_size\n",
        "        self.sampling_strategy = sampling_strategy\n",
        "        self.transform = transform\n",
        "\n",
        "        # Get all class folders in the split directory\n",
        "        self.classes = sorted([d for d in os.listdir(split_dir)\n",
        "                              if os.path.isdir(os.path.join(split_dir, d))])\n",
        "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
        "        print(f\"Found {len(self.classes)} classes\")\n",
        "\n",
        "        self.data = self._load_data()\n",
        "\n",
        "    def _random_sample_frames(self, all_frames, num_frames):\n",
        "        \"\"\"Random sampling using indices\"\"\"\n",
        "        num_available = len(all_frames)\n",
        "\n",
        "        if num_available >= num_frames:\n",
        "            random_indices = random.sample(range(num_available), num_frames)\n",
        "            return [all_frames[i] for i in random_indices]\n",
        "        else:\n",
        "            random_indices = random.choices(range(num_available), k=num_frames)\n",
        "            return [all_frames[i] for i in random_indices]\n",
        "\n",
        "    def _equidistant_sample_frames(self, all_frames, num_frames):\n",
        "        \"\"\"Equidistant sampling using indices\"\"\"\n",
        "        num_available = len(all_frames)\n",
        "\n",
        "        if num_available <= num_frames:\n",
        "            indices = []\n",
        "            for i in range(num_frames):\n",
        "                indices.append(i % num_available)\n",
        "            return [all_frames[i] for i in indices]\n",
        "        else:\n",
        "            step = num_available / num_frames\n",
        "            indices = [int(i * step) for i in range(num_frames)]\n",
        "            return [all_frames[i] for i in indices]\n",
        "\n",
        "    def _vivit_uniform_sample_frames(self, all_frames, num_frames, stride=2):\n",
        "        \"\"\"\n",
        "        ViViT paper uniform temporal sampling\n",
        "        Sample with stride (every 2nd frame) as described in paper\n",
        "        \"\"\"\n",
        "        num_available = len(all_frames)\n",
        "        total_needed = num_frames * stride\n",
        "\n",
        "        if num_available >= total_needed:\n",
        "            # Center crop and sample with stride\n",
        "            start_idx = (num_available - total_needed) // 2\n",
        "            indices = [start_idx + i * stride for i in range(num_frames)]\n",
        "            return [all_frames[i] for i in indices]\n",
        "        else:\n",
        "            # Uniform sampling across available frames\n",
        "            step = num_available / num_frames\n",
        "            indices = [int(i * step) for i in range(num_frames)]\n",
        "            return [all_frames[i] for i in indices]\n",
        "\n",
        "    def _load_data(self):\n",
        "        \"\"\"Load data from the split directory\"\"\"\n",
        "        data = []\n",
        "\n",
        "        for class_name in self.classes:\n",
        "            class_path = os.path.join(self.split_dir, class_name)\n",
        "            label = self.class_to_idx[class_name]\n",
        "\n",
        "            video_folders = [v for v in os.listdir(class_path)\n",
        "                           if os.path.isdir(os.path.join(class_path, v))]\n",
        "\n",
        "            for video_folder in video_folders:\n",
        "                video_path = os.path.join(class_path, video_folder)\n",
        "\n",
        "                if '_' in video_folder and video_folder.split('_')[-1].isdigit():\n",
        "                    continue\n",
        "\n",
        "                all_frames = sorted([f for f in os.listdir(video_path)\n",
        "                                   if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))])\n",
        "\n",
        "                if len(all_frames) == 0:\n",
        "                    continue\n",
        "\n",
        "                # Select sampling strategy\n",
        "                if self.sampling_strategy == 'random':\n",
        "                    clip_frames = self._random_sample_frames(all_frames, self.clip_size)\n",
        "                elif self.sampling_strategy == 'equidistant':\n",
        "                    clip_frames = self._equidistant_sample_frames(all_frames, self.clip_size)\n",
        "                elif self.sampling_strategy == 'vivit_uniform':\n",
        "                    clip_frames = self._vivit_uniform_sample_frames(all_frames, self.clip_size)\n",
        "                else:\n",
        "                    clip_frames = self._random_sample_frames(all_frames, self.clip_size)\n",
        "\n",
        "                data.append((video_path, label, 0, clip_frames))\n",
        "\n",
        "        print(f\"Total clips: {len(data)}\")\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def _get_fallback_frame(self, video_path):\n",
        "        \"\"\"Get a fallback frame when specific frame loading fails\"\"\"\n",
        "        available_frames = [f for f in os.listdir(video_path)\n",
        "                          if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
        "        if available_frames:\n",
        "            return Image.open(os.path.join(video_path, available_frames[0]))\n",
        "        else:\n",
        "            return Image.fromarray(np.zeros((224, 224, 3), dtype=np.uint8))\n",
        "\n",
        "    def _load_regular_frame(self, video_path, frame_name):\n",
        "        \"\"\"Load a regular frame from disk\"\"\"\n",
        "        frame_path = os.path.join(video_path, frame_name)\n",
        "        if os.path.exists(frame_path):\n",
        "            try:\n",
        "                return Image.open(frame_path)\n",
        "            except Exception as e:\n",
        "                return self._get_fallback_frame(video_path)\n",
        "        else:\n",
        "            return self._get_fallback_frame(video_path)\n",
        "\n",
        "    def _load_frames_from_clip(self, video_path, clip_frames):\n",
        "        \"\"\"Load frames for a specific clip\"\"\"\n",
        "        frames = []\n",
        "\n",
        "        for frame_name in clip_frames:\n",
        "            img = self._load_regular_frame(video_path, frame_name)\n",
        "            frames.append(img)\n",
        "\n",
        "        while len(frames) < self.clip_size:\n",
        "            if frames:\n",
        "                frames.append(frames[-1])\n",
        "            else:\n",
        "                frames.append(Image.fromarray(np.zeros((224, 224, 3), dtype=np.uint8)))\n",
        "\n",
        "        frames = frames[:self.clip_size]\n",
        "        return frames\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path, label, clip_idx, clip_frames = self.data[idx]\n",
        "\n",
        "        frames = self._load_frames_from_clip(video_path, clip_frames)\n",
        "\n",
        "        if self.transform:\n",
        "            frames = [self.transform(frame) for frame in frames]\n",
        "\n",
        "        return torch.stack(frames), label\n",
        "\n",
        "def get_vivit_dataloader(root_dir, batch_size=8, clip_size=32, sampling_strategy='vivit_uniform'):\n",
        "    \"\"\"\n",
        "    Create dataloaders for ViViT model\n",
        "\n",
        "    Args:\n",
        "        root_dir (str): Path to directory containing train, val, test folders\n",
        "        batch_size (int): Batch size (smaller for ViViT due to memory requirements)\n",
        "        clip_size (int): Number of frames per clip (32 for ViViT)\n",
        "        sampling_strategy (str): Sampling strategy to use\n",
        "\n",
        "    Returns:\n",
        "        tuple: (train_loader, val_loader, test_loader)\n",
        "    \"\"\"\n",
        "    # ViViT preprocessing - matches paper specifications\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    train_dir = os.path.join(root_dir, 'train')\n",
        "    val_dir = os.path.join(root_dir, 'val')\n",
        "    test_dir = os.path.join(root_dir, 'test')\n",
        "\n",
        "    for split_dir, name in [(train_dir, 'train'), (val_dir, 'val'), (test_dir, 'test')]:\n",
        "        if not os.path.exists(split_dir):\n",
        "            raise ValueError(f\"{name} directory not found at {split_dir}\")\n",
        "\n",
        "    train_dataset = ViViTHMDBDataset(\n",
        "        split_dir=train_dir,\n",
        "        clip_size=clip_size,\n",
        "        sampling_strategy=sampling_strategy,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    val_dataset = ViViTHMDBDataset(\n",
        "        split_dir=val_dir,\n",
        "        clip_size=clip_size,\n",
        "        sampling_strategy=sampling_strategy,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    test_dataset = ViViTHMDBDataset(\n",
        "        split_dir=test_dir,\n",
        "        clip_size=clip_size,\n",
        "        sampling_strategy=sampling_strategy,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=batch_size, shuffle=True,\n",
        "        num_workers=0, pin_memory=False\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, batch_size=batch_size, shuffle=False,\n",
        "        num_workers=0, pin_memory=False\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset, batch_size=batch_size, shuffle=False,\n",
        "        num_workers=0, pin_memory=False\n",
        "    )\n",
        "\n",
        "    print(f\"Train: {len(train_dataset)} | Val: {len(val_dataset)} | Test: {len(test_dataset)}\")\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "def load_vivit_model():\n",
        "    \"\"\"\n",
        "    Load the pre-trained ViViT model for video classification\n",
        "    \"\"\"\n",
        "    import logging\n",
        "    warnings.filterwarnings(\"ignore\", message=\"Some weights of VivitForVideoClassification were not initialized\")\n",
        "    logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\n",
        "\n",
        "    # HMDB-51 subset classes\n",
        "    label_index_dict = {\n",
        "        'brush_hair': 0, 'cartwheel': 1, 'catch': 2, 'chew': 3, 'climb': 4,\n",
        "        'climb_stairs': 5, 'draw_sword': 6, 'eat': 7, 'fencing': 8, 'flic_flac': 9,\n",
        "        'golf': 10, 'handstand': 11, 'kiss': 12, 'pick': 13, 'pour': 14,\n",
        "        'pullup': 15, 'pushup': 16, 'ride_bike': 17, 'shoot_bow': 18, 'shoot_gun': 19,\n",
        "        'situp': 20, 'smile': 21, 'smoke': 22, 'throw': 23, 'wave': 24\n",
        "    }\n",
        "\n",
        "    index_label_dict = {v: k for k, v in label_index_dict.items()}\n",
        "\n",
        "    # Load ViViT processor and model\n",
        "    try:\n",
        "        processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n",
        "        ckpt = \"google/vivit-b-16x2-kinetics400\"\n",
        "    except:\n",
        "        # Fallback if the exact model isn't available\n",
        "        print(\"Using alternative ViViT model...\")\n",
        "        processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n",
        "        ckpt = \"google/vivit-b-16x2-kinetics400\"\n",
        "\n",
        "    model = VivitForVideoClassification.from_pretrained(\n",
        "        ckpt,\n",
        "        label2id=label_index_dict,\n",
        "        id2label=index_label_dict,\n",
        "        ignore_mismatched_sizes=True\n",
        "    )\n",
        "\n",
        "    return processor, model\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Compute evaluation metrics\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    if isinstance(predictions, torch.Tensor):\n",
        "        predictions = predictions.cpu().numpy()\n",
        "    if isinstance(labels, torch.Tensor):\n",
        "        labels = labels.cpu().numpy()\n",
        "\n",
        "    # Top-1 accuracy\n",
        "    top1_predictions = np.argmax(predictions, axis=1)\n",
        "    top1_accuracy = np.mean(top1_predictions == labels)\n",
        "\n",
        "    # Top-5 accuracy\n",
        "    top5_predictions = np.argsort(predictions, axis=1)[:, -5:]\n",
        "    top5_correct = np.array([labels[i] in top5_predictions[i] for i in range(len(labels))])\n",
        "    top5_accuracy = np.mean(top5_correct)\n",
        "\n",
        "    return {\n",
        "        \"top1_accuracy\": top1_accuracy,\n",
        "        \"top5_accuracy\": top5_accuracy,\n",
        "        \"eval_top1_accuracy\": top1_accuracy,\n",
        "        \"eval_top5_accuracy\": top5_accuracy,\n",
        "    }\n",
        "\n",
        "def train_vivit_model(data_dir, epochs=8, batch_size=8, learning_rate=0.1,\n",
        "                     clip_size=32, sampling_strategy='vivit_uniform', optimizer_type='sgd'):\n",
        "    \"\"\"\n",
        "    Train ViViT model with Trainer API\n",
        "\n",
        "    Args:\n",
        "        data_dir (str): Path to dataset directory\n",
        "        epochs (int): Number of training epochs\n",
        "        batch_size (int): Batch size (small for ViViT due to memory)\n",
        "        learning_rate (float): Learning rate (lower for ViViT)\n",
        "        clip_size (int): Number of frames per clip\n",
        "        sampling_strategy (str): Temporal sampling strategy\n",
        "        optimizer_type (str): 'adamw' or 'sgd'\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    print(f\"ViViT Configuration:\")\n",
        "    print(f\"  - Frames per clip: {clip_size}\")\n",
        "    print(f\"  - Batch size: {batch_size}\")\n",
        "    print(f\"  - Learning rate: {learning_rate}\")\n",
        "    print(f\"  - Sampling strategy: {sampling_strategy}\")\n",
        "    print(f\"  - Optimizer: {optimizer_type}\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Load data with ViViT-specific parameters\n",
        "    train_loader, val_loader, test_loader = get_vivit_dataloader(\n",
        "        data_dir,\n",
        "        batch_size=batch_size,\n",
        "        clip_size=clip_size,\n",
        "        sampling_strategy=sampling_strategy\n",
        "    )\n",
        "\n",
        "    processor, model = load_vivit_model()\n",
        "\n",
        "    train_dataset = train_loader.dataset\n",
        "    val_dataset = val_loader.dataset\n",
        "\n",
        "    def data_collator(batch):\n",
        "        videos = torch.stack([item[0] for item in batch])\n",
        "        labels = torch.tensor([item[1] for item in batch])\n",
        "        return {\n",
        "            'pixel_values': videos,\n",
        "            'labels': labels\n",
        "        }\n",
        "\n",
        "    # Choose optimizer\n",
        "    if optimizer_type.lower() == 'adamw':\n",
        "        optimizer = optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=learning_rate,\n",
        "            weight_decay=1e-4,\n",
        "            betas=(0.9, 0.999)\n",
        "        )\n",
        "    else:  # SGD\n",
        "        optimizer = optim.SGD(\n",
        "            model.parameters(),\n",
        "            lr=learning_rate,\n",
        "            momentum=0.9,\n",
        "            weight_decay=1e-3\n",
        "        )\n",
        "\n",
        "    # Create output directory\n",
        "    output_dir = f\"/content/drive/MyDrive/AML/vivit_results_{sampling_strategy}_{optimizer_type}\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    os.makedirs(os.path.join(output_dir, 'logs'), exist_ok=True)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        overwrite_output_dir=True,\n",
        "        eval_strategy='epoch',\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        num_train_epochs=epochs,\n",
        "        logging_dir=os.path.join(output_dir, 'logs'),\n",
        "        logging_strategy=\"epoch\",\n",
        "        save_strategy='epoch',\n",
        "        save_total_limit=1,\n",
        "        remove_unused_columns=False,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model='eval_top1_accuracy',\n",
        "        greater_is_better=True,\n",
        "        report_to=\"tensorboard\",\n",
        "        push_to_hub=False,\n",
        "        save_only_model=True,\n",
        "        run_name=f\"vivit_{sampling_strategy}_{optimizer_type}\",\n",
        "        gradient_accumulation_steps=4,  # Help with small batch size\n",
        "        fp16=True,  # Mixed precision for memory efficiency\n",
        "        dataloader_pin_memory=False,\n",
        "        dataloader_num_workers=0,\n",
        "    )\n",
        "\n",
        "    early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=4)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        optimizers=(optimizer, None),\n",
        "        eval_dataset=val_dataset,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[early_stopping_callback]\n",
        "    )\n",
        "\n",
        "    print(\"Starting ViViT training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    print(\"\\nEvaluating ViViT on test set...\")\n",
        "    test_dataset = test_loader.dataset\n",
        "    test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
        "\n",
        "    print(f\"\\n=== FINAL ViViT TEST RESULTS ===\")\n",
        "    print(f\"Configuration: {clip_size} frames, {sampling_strategy} sampling, {optimizer_type} optimizer\")\n",
        "    print(f\"Test Top-1 Accuracy: {test_results.get('eval_top1_accuracy', 'N/A'):.4f}\")\n",
        "    print(f\"Test Top-5 Accuracy: {test_results.get('eval_top5_accuracy', 'N/A'):.4f}\")\n",
        "    print(f\"Test Loss: {test_results.get('eval_loss', 'N/A'):.4f}\")\n",
        "\n",
        "    # Save model\n",
        "    model_save_path = f\"/content/drive/MyDrive/AML/vivit_model_{sampling_strategy}_{optimizer_type}\"\n",
        "    trainer.save_model(model_save_path)\n",
        "\n",
        "    # Save results\n",
        "    results_save_path = f\"/content/drive/MyDrive/AML/vivit_results_{sampling_strategy}_{optimizer_type}.json\"\n",
        "    with open(results_save_path, \"w\") as f:\n",
        "        json.dump(test_results, f, indent=2)\n",
        "\n",
        "    print(f\"ViViT training complete.\")\n",
        "    print(f\"Model saved to: {model_save_path}\")\n",
        "    print(f\"Results saved to: {results_save_path}\")\n",
        "\n",
        "    return test_results\n",
        "\n",
        "# Clean up before training\n",
        "def setup_vivit_training():\n",
        "    \"\"\"Setup environment for ViViT training\"\"\"\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"GPU memory cleared for ViViT training\")\n",
        "\n",
        "# Usage Examples:\n",
        "\n",
        "def train_vivit_paper_config():\n",
        "    setup_vivit_training()\n",
        "    return train_vivit_model(\n",
        "        data_dir='/content/drive/MyDrive/AML/dataset',\n",
        "        epochs=8,\n",
        "        batch_size=8,  # Small due to 32 frames\n",
        "        learning_rate=0.1,\n",
        "        clip_size=32,  # ViViT paper uses 32 frames\n",
        "        sampling_strategy='vivit_uniform',\n",
        "        optimizer_type='sgd'\n",
        "    )\n",
        "train_vivit_paper_config()"
      ],
      "metadata": {
        "id": "K-GSNF--Op2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 974
        },
        "outputId": "4a8f4986-54cf-43d9-87f0-0272e8193b2a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU memory cleared for ViViT training\n",
            "Using device: cuda\n",
            "ViViT Configuration:\n",
            "  - Frames per clip: 32\n",
            "  - Batch size: 8\n",
            "  - Learning rate: 0.1\n",
            "  - Sampling strategy: vivit_uniform\n",
            "  - Optimizer: sgd\n",
            "Found 25 classes\n",
            "Total clips: 900\n",
            "Found 25 classes\n",
            "Total clips: 225\n",
            "Found 25 classes\n",
            "Total clips: 125\n",
            "Train: 900 | Val: 225 | Test: 125\n",
            "Starting ViViT training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='232' max='232' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [232/232 26:14, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Top1 Accuracy</th>\n",
              "      <th>Top5 Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.359500</td>\n",
              "      <td>0.762734</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.951111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.468400</td>\n",
              "      <td>0.926487</td>\n",
              "      <td>0.702222</td>\n",
              "      <td>0.960000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.278100</td>\n",
              "      <td>0.776938</td>\n",
              "      <td>0.760000</td>\n",
              "      <td>0.960000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.109700</td>\n",
              "      <td>0.588663</td>\n",
              "      <td>0.844444</td>\n",
              "      <td>0.973333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.076000</td>\n",
              "      <td>0.560717</td>\n",
              "      <td>0.817778</td>\n",
              "      <td>0.968889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.029300</td>\n",
              "      <td>0.585824</td>\n",
              "      <td>0.835556</td>\n",
              "      <td>0.968889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.010800</td>\n",
              "      <td>0.559441</td>\n",
              "      <td>0.853333</td>\n",
              "      <td>0.960000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.002300</td>\n",
              "      <td>0.536043</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.968889</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating ViViT on test set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16/16 00:16]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== FINAL ViViT TEST RESULTS ===\n",
            "Configuration: 32 frames, vivit_uniform sampling, sgd optimizer\n",
            "Test Top-1 Accuracy: 0.8880\n",
            "Test Top-5 Accuracy: 0.9920\n",
            "Test Loss: 0.4021\n",
            "ViViT training complete.\n",
            "Model saved to: /content/drive/MyDrive/AML/vivit_model_vivit_uniform_sgd\n",
            "Results saved to: /content/drive/MyDrive/AML/vivit_results_vivit_uniform_sgd.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_top1_accuracy': 0.888,\n",
              " 'eval_top5_accuracy': 0.992,\n",
              " 'eval_loss': 0.4020753502845764,\n",
              " 'eval_runtime': 19.1352,\n",
              " 'eval_samples_per_second': 6.532,\n",
              " 'eval_steps_per_second': 0.836,\n",
              " 'epoch': 8.0}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "buOIeCVjO0F9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}